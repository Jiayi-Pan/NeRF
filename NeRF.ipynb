{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# select devices\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Good to go!\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Bad to go!\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load config from config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load config\n",
    "sys.path.append(os.getcwd())\n",
    "# choose between ship, lego\n",
    "import configs.ship, configs.lego\n",
    "sample_t: tuple = (2,6)\n",
    "# change config file here\n",
    "config = configs.lego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load the dataset and show the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nerf.data import load_blender\n",
    "imgs, poses, int_mat = load_blender(config.datadir, device=DEVICE, scale_factor=2)\n",
    "img_n, img_h, img_w = imgs.shape[:3]\n",
    "# visualize\n",
    "plt.imshow(np.array(imgs[0].to(device=\"cpu\")))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"demo image\")\n",
    "plt.show()\n",
    "print(\"and its pose: \")\n",
    "print(np.array(poses[0].to(device=\"cpu\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute rays\n",
    "from nerf.graphics import compute_rays\n",
    "\n",
    "rays_o, rays_d = compute_rays((img_h, img_w), int_mat, poses[0])\n",
    "print(\"origin: \", rays_o[0,0])\n",
    "print(\"normalized origin: \", F.normalize(rays_o[0,0], dim=0))\n",
    "print(\"center of ray: \", rays_d[img_h//2,img_w//2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query from rays\n",
    "from nerf.graphics import queries_from_rays\n",
    "samples = None\n",
    "samples, depths = queries_from_rays(rays_o, rays_d, sample_t, 8)\n",
    "print(\"samples[0, 0]: \", samples[0,0])\n",
    "print(\"depths: \", depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test posencode\n",
    "\n",
    "from nerf.nerf_helper import PosEncode\n",
    "\n",
    "L = 2\n",
    "x = torch.tensor([[1.1, 1.2, 1.3], [2.1, 2.2, 2.3]])\n",
    "enc_x = PosEncode(x, L, True)\n",
    "\n",
    "print(enc_x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test render from nerf\n",
    "from nerf.graphics import render_from_nerf\n",
    "fake_depth = torch.Tensor([1])\n",
    "fake_nerf_output = imgs[0].cpu().reshape(img_h, img_w, 1, 4)\n",
    "rgb, depth = render_from_nerf(fake_nerf_output, fake_depth)\n",
    "plt.imshow(rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One iteration of TinyNeRF (forward pass).\n",
    "# TODO train\n",
    "# raise Exception(\"nothing wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf.model import NeRF, TinyNeRF\n",
    "from nerf.nerf_helper import nerf_iter_once, tinynerf_iter_once\n",
    "import os.path\n",
    "\n",
    "# parameters\n",
    "L_pos = 10\n",
    "L_dir = 4\n",
    "depth_samples_per_ray = 32\n",
    "\n",
    "chunksize = 8192\n",
    "lr = 5e-3\n",
    "betas=(0.9, 0.999)\n",
    "num_it = 2000\n",
    "display_every = 10\n",
    "\n",
    "# load validation data\n",
    "imgs_val, poses_val, int_mat_val = load_blender(config.datadir, data_type=\"val\",scale_factor=2, device=DEVICE)\n",
    "num_val = imgs_val.shape[0]\n",
    "\n",
    "# models\n",
    "# model = NeRF(ch_in_pos=6*L_pos, ch_in_dir=6*L_dir, fc_width=64, fc_depth=4, skips=[2])\n",
    "model = TinyNeRF(6*L_pos+3, fc_width=128)\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    betas=betas,\n",
    ")\n",
    "ckpt_path = 'nerf.pt'\n",
    "\n",
    "# train\n",
    "psnrs = []\n",
    "its = []\n",
    "i = 0\n",
    "# check saved checkpoints\n",
    "# if os.path.exists(ckpt_path):\n",
    "if False:\n",
    "    print(\"checkpoint found! Loading...\")\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    i = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    psnrs = checkpoint['prnrs']\n",
    "    print(\"checkpoint loaded, i =\",i)\n",
    "else:\n",
    "    print(\"No checkpoint found\")\n",
    "\n",
    "for i in range(i, num_it):\n",
    "    gt_img_idx = np.random.randint(5)\n",
    "    gt_img = imgs[gt_img_idx].to(DEVICE)\n",
    "    gt_c2w = poses[gt_img_idx]\n",
    "\n",
    "    pred_rgb,_ = tinynerf_iter_once(\n",
    "                model,\n",
    "                (img_h, img_w),\n",
    "                int_mat,\n",
    "                gt_c2w,\n",
    "                sample_t,\n",
    "                num_samples=4\n",
    "                )\n",
    "    \n",
    "    loss = torch.nn.functional.mse_loss(pred_rgb, gt_img[...,:3], reduction='sum')\n",
    "\n",
    "    # plt.imshow(pred_rgb.detach().cpu().numpy())\n",
    "    # plt.show()\n",
    "    # plt.imshow(gt_img[...,:3].detach().cpu().numpy())\n",
    "    # plt.show()\n",
    "    print(\"train_it:\", i, \"img_idx: \", gt_img_idx, \"loss:\",float(loss))\n",
    "    print(\"pred[100,100]:\",pred_rgb[100,100])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % display_every == 0:\n",
    "        val_idx = 0\n",
    "        val_img = imgs[val_idx].to(DEVICE)\n",
    "        val_c2w = poses[val_idx]\n",
    "        # val_idx = np.random.randint(num_val)\n",
    "        # val_img = imgs_val[val_idx].to(DEVICE)\n",
    "        # val_c2w = poses_val[val_idx]\n",
    "\n",
    "        pred_rgb,_ = tinynerf_iter_once(\n",
    "                model,\n",
    "                (img_h, img_w),\n",
    "                int_mat,\n",
    "                val_c2w,\n",
    "                sample_t,\n",
    "                num_samples=4\n",
    "                )\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(pred_rgb, val_img[...,:3])\n",
    "        print(\"Iteration \", i)\n",
    "        print(\"Val loss: \", loss)\n",
    "\n",
    "        psnr = -10. * torch.log10(loss)\n",
    "        psnrs.append(psnr.item())\n",
    "        its.append(i)\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(121)\n",
    "        img_np = pred_rgb.detach().cpu().numpy()\n",
    "        plt.imshow(img_np)\n",
    "        plt.title(f\"Iteration {i}\")\n",
    "        plt.subplot(122)\n",
    "        plt.plot(its, psnrs)\n",
    "        plt.title(\"PSNR\")\n",
    "        plt.show()\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': i,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            'psnr': psnrs\n",
    "            }, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
