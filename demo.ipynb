{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini NeRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Novel view synthesis an important problem in computer vision. It has a lot of applications such as video enhancement and virtual reality. *Neural Radiance Fields* (NeRFs) is a simple and powerful model for such kind of problem. It use the idea of **volume rendering** to train a model that implicitly contains the 3D information and samples from it to reconstruct the image. By encoding both position and viewing direction into the input, the NeRF model is able to \"understand\" the radiance distribution in the scene, thus reconstructing the image with better illumination details.\n",
    "\n",
    "In this project, we reimplemented the NeRF method using pytorch and train it with smaller MLP and lower resolution images using only the \"coarse\" model so that it can quickly converge on smaller GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# select device\n",
    "DEVICE = None\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Good to go!\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Bad to go!\")\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "# load config files\n",
    "sys.path.append(os.getcwd())\n",
    "import configs.hotdog, configs.lego\n",
    "dataset_configs:list = [configs.hotdog, configs.lego]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters for model\n",
    "sample_t: tuple = (2,6) # t_n, t_f\n",
    "\n",
    "# 100x100: scale_factor=3\n",
    "# 200x200:scale_factor=2\n",
    "scale_factor = 3\n",
    "\n",
    "# dataset to use\n",
    "config = configs.lego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We are using two scenes from the datasets of the NeRF paper: the lego model and the hotdog. These datasets are generated from Blender so that the ground truth intrinsic and extrinsic parameters are known. The images are resized to 100x100 for smaller GPU. \n",
    "\n",
    "Below are examples extracted from the dataset. We can see that the lego model contains many details while the hotdog model is simpler but contains rich illumination features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf.data import load_blender\n",
    "imgs_dict:dict = {}\n",
    "poses_dict:dict = {}\n",
    "int_mat_dict:dict = {}\n",
    "demo_images: list = []\n",
    "names: list = []\n",
    "for data_config in dataset_configs:\n",
    "    key:str = data_config.expname\n",
    "    imgs_dict[key], poses_dict[key], int_mat_dict[key] = load_blender(data_config.datadir, device=\"cpu\", scale_factor=scale_factor)\n",
    "    demo_images.append(imgs_dict[key][0])\n",
    "    names.append(data_config.expname)\n",
    "\n",
    "# plot images\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "columns = 2\n",
    "for i, image in enumerate(demo_images):\n",
    "    ax = plt.subplot(int(len(demo_images) / columns + 1), int(columns), i + 1)\n",
    "    ax.title.set_text(names[i])\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "# rename data for further usage\n",
    "imgs, poses, int_mat = imgs_dict[config.expname], poses_dict[config.expname], int_mat_dict[config.expname]\n",
    "img_h, img_w = imgs.shape[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "In this section, we showed several functions used during training and inference, and tested them with fake data input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray Generation\n",
    "This function generate a vector indicating a ray from each of the pixels of an image and transform them into the world coordinate using camera extrinsics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute rays\n",
    "from nerf.graphics import compute_rays\n",
    "\n",
    "rays_o, rays_d = compute_rays((img_h, img_w), int_mat, poses[0])\n",
    "print(\"origin: \", rays_o[0,0])\n",
    "print(\"center of ray: \", rays_d[img_h//2,img_w//2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from ray\n",
    "\n",
    "This function generate samples from rays using the coarse sampling method described from the paper: divide the ray in a range to equally size bins, sample uniformly from each bin to get a set of sample points. For training time consideration, we did not implemented the fine model.\n",
    "\n",
    "The test block extracted samples from one ray, we can see that the samples are distributed on a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf.graphics import queries_from_rays\n",
    "samples, depths = queries_from_rays(rays_o, rays_d, sample_t, 8)\n",
    "print(\"samples[0, 0]: \", samples[0,0])\n",
    "print(\"depths[0, 0]: \", depths[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "The author of the paper found that the model will be very hard to converge if the positions and directions are passed directly into the model. Therefore, a encoding function is needed to map the input to higher dimension space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf.nerf_helper import PosEncode\n",
    "\n",
    "L = 6\n",
    "x = torch.tensor([[ 1.8013, -0.6242,  0.7009]])\n",
    "enc_x = PosEncode(x, L, True)\n",
    "print(enc_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering\n",
    "The NeRF model will output the rgb values and the volume densities at the sampled points. A rendering function is needed to combine these results together and calculate the final rgbd value displaying on the pixel.\n",
    "\n",
    "The testing block use the original training image as an fake output of the MLP and the render function successfully recovered the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf.graphics import render_from_nerf\n",
    "fake_depth = torch.Tensor([1])\n",
    "fake_nerf_output = imgs[0].cpu().reshape(img_h, img_w, 1, 4)\n",
    "rgb, depth = render_from_nerf(fake_nerf_output, fake_depth)\n",
    "plt.imshow(rgb)\n",
    "plt.title(\"Recovered image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating poses\n",
    "This function is used when we want to view the model from a new perspective (pose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf.graphics import generate_demo_poses\n",
    "demo_poses = generate_demo_poses()\n",
    "print(demo_poses.shape)\n",
    "print(demo_poses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "In this section we will show a demo process of training and compare the pretrained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf.model import NeRF \n",
    "import os.path\n",
    "\n",
    "seed = 9458\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "###### hyper-parameters\n",
    "L_pos = 10\n",
    "L_dir = 4\n",
    "num_samples = 32\n",
    "batch_size = 8192 # increase batchsize if u have large GPU MEM\n",
    "fc_width = 128\n",
    "fc_depth = 4\n",
    "skips = [2]\n",
    "lr = 5e-4\n",
    "num_it = 10001\n",
    "display_every = 200\n",
    "\n",
    "###### models\n",
    "model = NeRF(ch_in_pos=6*L_pos+3, ch_in_dir=6*L_dir+3, fc_width=fc_width, fc_depth=fc_depth, skips=skips)\n",
    "model.to(DEVICE)\n",
    "\n",
    "###### load validation data\n",
    "imgs_val, poses_val, _ = load_blender(config.datadir, data_type=\"val\",scale_factor=scale_factor, device=\"cpu\")\n",
    "num_val = imgs_val.shape[0]\n",
    "\n",
    "###### train\n",
    "psnrs = []\n",
    "val_its = []\n",
    "i = 0\n",
    "\n",
    "###### optimizer, checkpoint\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=lr)\n",
    "ckpt_path = 'nerf.pt'\n",
    "\n",
    "###### check saved checkpoints\n",
    "if os.path.exists(ckpt_path):\n",
    "    print(\"checkpoint found! Loading...\")\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    i = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    psnrs = checkpoint['psnrs']\n",
    "    val_its = checkpoint['its']\n",
    "    print(\"checkpoint loaded, i =\",i)\n",
    "else:\n",
    "    print(\"No checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "from nerf.training_logic import train_NeRF\n",
    "train_NeRF(model = model, optimizer=optimizer,imgs_train=imgs, imgs_val=imgs_val, poses_train=poses, poses_val=poses_val,int_mat=int_mat, sample_t=sample_t,\n",
    "            L_pos=L_pos, L_dir=L_dir, num_samples=num_samples, ckpt_path=ckpt_path, batch_size=batch_size,\n",
    "            psnrs=psnrs, val_its=val_its, start_iter_num=i, end_iter_num=num_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and generate video\n",
    "After training we can generate a demo video using generated pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from nerf.nerf_helper import nerf_iter_once\n",
    "import cv2\n",
    "import imageio\n",
    "# generate pose\n",
    "model.eval()\n",
    "gen_num: int = 120\n",
    "repeat: int = 2\n",
    "gen_poses:Tensor = generate_demo_poses(height=4, num_poses=gen_num).to(poses).to(DEVICE)\n",
    "gen_imgs:list = []\n",
    "for i in range(gen_num):\n",
    "    with torch.no_grad():\n",
    "        pred_rgb, pred_depth = nerf_iter_once(\n",
    "                model,\n",
    "                (img_h, img_w),\n",
    "                int_mat.to(DEVICE),\n",
    "                gen_poses[i],\n",
    "                sample_t,\n",
    "                L_pos,\n",
    "                L_dir,\n",
    "                num_samples=num_samples,\n",
    "                batch_size=batch_size\n",
    "                )\n",
    "    # concat channels\n",
    "    pred_rgbd: Tensor = torch.cat([pred_rgb, pred_depth[...,None]], dim=-1)\n",
    "    # translate to [0,255]\n",
    "    img_np = np.array(pred_rgbd.detach().cpu()*255).astype(np.uint8)\n",
    "    img_np = cv2.resize(img_np, (112,112), interpolation=cv2.INTER_AREA)\n",
    "    gen_imgs.append(img_np)\n",
    "\n",
    "gen_imgs = gen_imgs * repeat\n",
    "\n",
    "demo_video_path: str = \"media/{}.mp4\".format(config.expname)\n",
    "imageio.mimwrite(demo_video_path, gen_imgs, fps=30, quality=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Video\n",
    "video = Video.from_file(demo_video_path)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Comparison\n",
    "We perform inference based on the poses of the test images and compare the results with the ground truth. The results aer shown below.\n",
    "\n",
    "lego_100x100 on testdata:\n",
    "PSNR:  25.975324880729126\n",
    "SSIM:  0.8976107119105973\n",
    "\n",
    "Hotdog on test dataset:\n",
    "PSNR:  29.81981982968555\n",
    "SSIM:  0.9356882317837466"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_msssim\n",
    "def test():\n",
    "    psnr_ = 0\n",
    "    ssim_ = 0\n",
    "    for i in range(num_test):\n",
    "        test_idx = i\n",
    "        test_img = imgs_test[test_idx].clone().to(DEVICE)\n",
    "        test_c2w = poses_test[test_idx].clone().to(DEVICE)\n",
    "\n",
    "        pred_rgb, _ = nerf_iter_once(\n",
    "                    model,\n",
    "                    (img_h, img_w),\n",
    "                    int_mat_test.to(DEVICE),\n",
    "                    test_c2w,\n",
    "                    sample_t,\n",
    "                    L_pos,\n",
    "                    L_dir,\n",
    "                    num_samples=num_samples,\n",
    "                    batch_size=batch_size\n",
    "                )\n",
    "        loss = torch.nn.functional.mse_loss(pred_rgb, test_img[..., :3])\n",
    "\n",
    "        psnr_ += float(-10. * torch.log10(loss))\n",
    "        \n",
    "        ssim_val = pytorch_msssim.ssim(pred_rgb.permute(2,0,1).reshape(1,3,200,200), test_img[..., :3].permute(2,0,1).reshape(1,3,200,200), data_range=1)\n",
    "        ssim_ += float(ssim_val)\n",
    "\n",
    "    psnr_ /= num_test\n",
    "    ssim_ /= num_test\n",
    "\n",
    "    print(\"PSNR: \", psnr_)\n",
    "    print(\"SSIM: \", ssim_)\n",
    "\n",
    "    \n",
    "imgs_test, poses_test, int_mat_test = load_blender(config.datadir, data_type=\"test\",scale_factor=scale_factor, device=\"cpu\")\n",
    "num_test = imgs_val.shape[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results with ground truth image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf.nerf_helper import compare\n",
    "data_pac:dict = {\n",
    "        \"imgs\": imgs,\n",
    "        \"poses\": poses,\n",
    "        \"DEVICE\": DEVICE,\n",
    "        \"config\": config,\n",
    "        \"scale_factor\": scale_factor,\n",
    "        \"sample_t\": sample_t,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"model\": model,\n",
    "        \"L_pos\": L_pos,\n",
    "        \"L_dir\": L_dir\n",
    "}\n",
    "ckpt_path = './trained_models/basic_nerf_hotdog_9800.pt'\n",
    "if os.path.exists(ckpt_path):\n",
    "    print(ckpt_path, \"found! Loading...\")\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    i = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    psnrs = checkpoint['psnrs']\n",
    "    its = checkpoint['its']\n",
    "    print(\"checkpoint loaded, i =\",i)\n",
    "else:\n",
    "    print(\"No checkpoint found\")\n",
    "    \n",
    "with torch.no_grad():\n",
    "    compare(data_pac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./media/lego_cmp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this project, we've reimplemented the basic functionality of NeRF in pyTorch, applied it to two scenarios and done both quantitively and qualitative analysis on the result.\n",
    "\n",
    "\n",
    "\n",
    "The major shortcoming of this project is the lack of hierarchical sampling and the much lower resolution we select.  If given more time and computation resources, we would consider to fully reimplement NeRF, train it with standard resolution, and even try something more interesting, like [NeRF without Neural Networks](https://alexyu.net/plenoxels/)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5096599c657fdf4d0f877628e64cdb82d492464e9a0dff3905aabc06f60ed6d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nerf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
